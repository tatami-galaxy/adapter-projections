

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Adapter Activation and Composition &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Adapter Training" href="training.html" />
    <link rel="prev" title="Introduction to Adapters" href="adapters.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">adapter-transformers</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="adapters.html">Introduction to Adapters</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Adapter Activation and Composition</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#stack"><code class="docutils literal notranslate"><span class="pre">Stack</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#fuse"><code class="docutils literal notranslate"><span class="pre">Fuse</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#split"><code class="docutils literal notranslate"><span class="pre">Split</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#batchsplit"><code class="docutils literal notranslate"><span class="pre">BatchSplit</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#parallel"><code class="docutils literal notranslate"><span class="pre">Parallel</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#nesting-composition-blocks">Nesting composition blocks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="v2_transition.html">Transitioning from v1 to v2</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Hub</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="huggingface_hub.html">Integration with HuggingFace’s Model Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_layer.html">AdapterLayerBaseMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_utils.html">Adapter Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/weights_loaders.html">Weights Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xlmroberta.html">XLM-RoBERTa</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Adapter Activation and Composition</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/adapter_composition.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="adapter-activation-and-composition">
<h1>Adapter Activation and Composition<a class="headerlink" href="#adapter-activation-and-composition" title="Permalink to this headline">¶</a></h1>
<p>One of the great advantages of using adapters is the possibility to combine multiple adapters trained on different tasks in various ways.
To enable such adapter compositions, <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> comes with a modular and flexible concept to define how the input to the model should flow through the available adapters.
This not only allows stacking (<a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf"><em>MAD-X</em></a>) and fusing (<a class="reference external" href="https://arxiv.org/pdf/2005.00247.pdf"><em>AdapterFusion</em></a>) adapters, but also even more complex adapter setups.</p>
<p>The single location where all the adapter composition magic happens is the <code class="docutils literal notranslate"><span class="pre">active_adapters</span></code> property of the model class.
In the simplest case, you can set the name of a single adapter here to activate it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="s2">&quot;adapter_name&quot;</span>
</pre></div>
</div>
<p>Note that we also could have used <code class="docutils literal notranslate"><span class="pre">model.set_active_adapters(&quot;adapter_name&quot;)</span></code> which does the same.</p>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. important::
    ``active_adapters`` defines which of the available adapters are used in each forward and backward pass through the model. This means:

    - You cannot activate an adapter not previously added to the model using either ``add_adapter()`` or ``load_adapter()``.
    - All adapters not mentioned anywhere in the ``active_adapters`` setup are ignored although they might be loaded into the model. Thus, after adding an adapter, make sure to activate it.
</pre></div>
</div>
<p>The basic building blocks of the more advanced setups are simple objects derived from <code class="docutils literal notranslate"><span class="pre">AdapterCompositionBlock</span></code>,
each representing a different possibility to combine single adapters.
They are presented in more detail in the following.</p>
<div class="section" id="stack">
<h2><code class="docutils literal notranslate"><span class="pre">Stack</span></code><a class="headerlink" href="#stack" title="Permalink to this headline">¶</a></h2>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/stacking_adapters.png
    :height: 300
    :align: center
    :alt: Illustration of stacking adapters.

    Stacking adapters using the &#39;Stack&#39; block.
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Stack</span></code> block can be used to stack multiple adapters on top of each other.
This kind of adapter composition is used e.g. in the <em>MAD-X</em> framework for cross-lingual transfer <a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">(Pfeiffer et al., 2020)</a>, where language and task adapters are stacked on top of each other.
For more, check out <a class="reference external" href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb">this Colab notebook</a> on cross-lingual transfer.</p>
<p>In the following example, we stack the adapters <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code> so that in each layer, the input is first passed through <code class="docutils literal notranslate"><span class="pre">a</span></code>, the output of <code class="docutils literal notranslate"><span class="pre">a</span></code> is then inputted to <code class="docutils literal notranslate"><span class="pre">b</span></code> and the output of <code class="docutils literal notranslate"><span class="pre">b</span></code> is finally inputted to <code class="docutils literal notranslate"><span class="pre">c</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers.adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="o">//</span> <span class="o">...</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In v1.x of <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code>, stacking adapters was done using a list of adapter names, i.e. the example from above would be defined as <code class="docutils literal notranslate"><span class="pre">[&quot;a&quot;,</span> <span class="pre">&quot;b&quot;,</span> <span class="pre">&quot;c&quot;]</span></code>.
For backwards compatibility, you can still do this, although it is recommended to use the new syntax.</p>
</div>
<div class="section" id="fuse">
<h2><code class="docutils literal notranslate"><span class="pre">Fuse</span></code><a class="headerlink" href="#fuse" title="Permalink to this headline">¶</a></h2>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/Fusion.png
    :height: 300
    :align: center
    :alt: Illustration of AdapterFusion.

    Fusing adapters with AdapterFusion.
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Fuse</span></code> block can be used to activate a fusion layer of adapters.
<em>AdapterFusion</em> is a non-destructive way to combine the knowledge of multiple pre-trained adapters on a new downstream task, proposed by <a class="reference external" href="https://arxiv.org/pdf/2005.00247.pdf">Pfeiffer et al., 2021</a>.
In the following example, we activate the adapters <code class="docutils literal notranslate"><span class="pre">d</span></code>, <code class="docutils literal notranslate"><span class="pre">e</span></code> and <code class="docutils literal notranslate"><span class="pre">f</span></code> as well as the fusion layer that combines the outputs of all three.
The fusion layer is added beforehand using <code class="docutils literal notranslate"><span class="pre">model.add_adapter_fusion()</span></code> where we specify the names of the adapters which should be fused.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers.adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="o">//</span> <span class="o">...</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;d&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;e&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;f&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter_fusion</span><span class="p">([</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">Fuse</span><span class="p">(</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. important::
    Fusing adapters with the ``Fuse`` block only works successfully if an adapter fusion layer combining all of the adapters listed in the ``Fuse`` has been added to the model.
    This can be done either using ``add_adapter_fusion()`` or ``load_adapter_fusion()``.
</pre></div>
</div>
<p>To learn how training an <em>AdapterFusion</em> layer works, check out <a class="reference external" href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb">this Colab notebook</a> from the <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> repo.</p>
<p>In v1.x of <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code>, fusing adapters was done using a nested list of adapter names, i.e. the example from above would be defined as <code class="docutils literal notranslate"><span class="pre">[[&quot;d&quot;,</span> <span class="pre">&quot;e&quot;,</span> <span class="pre">&quot;f&quot;]]</span></code>.
For backwards compatibility, you can still do this, although it is recommended to use the new syntax.</p>
</div>
<div class="section" id="split">
<h2><code class="docutils literal notranslate"><span class="pre">Split</span></code><a class="headerlink" href="#split" title="Permalink to this headline">¶</a></h2>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/splitting_adapters.png
    :height: 300
    :align: center
    :alt: Illustration of splitting adapters.

    Splitting the input between two adapters using the &#39;Stack&#39; block.
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Split</span></code> block can be used to split an input sequence between two adapters.
This is done by specifying a split index, at which the sequences should be divided.
In the following example, we split each input sequence between adapters <code class="docutils literal notranslate"><span class="pre">g</span></code> and <code class="docutils literal notranslate"><span class="pre">h</span></code>.
For each sequence, all tokens from 0 up to 63 are forwarded through <code class="docutils literal notranslate"><span class="pre">g</span></code> while all tokens beginning at index 64 are forwarded through <code class="docutils literal notranslate"><span class="pre">h</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers.adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="o">//</span> <span class="o">...</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">split_index</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="batchsplit">
<h2><code class="docutils literal notranslate"><span class="pre">BatchSplit</span></code><a class="headerlink" href="#batchsplit" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">BatchSplit</span></code> lock is an alternative to split the input between several adapters. It does not split the input sequences but the
batch into smaller batches. As a result, the input sequences remain untouched.</p>
<p>In the following example, we split the batch between adapters <code class="docutils literal notranslate"><span class="pre">i</span></code>, <code class="docutils literal notranslate"><span class="pre">k</span></code> and <code class="docutils literal notranslate"><span class="pre">l</span></code>. The <code class="docutils literal notranslate"><span class="pre">batch_sizes</span></code>parameter specifies
the batch size for each of the adapters. The adapter <code class="docutils literal notranslate"><span class="pre">i</span></code> gets two sequences, <code class="docutils literal notranslate"><span class="pre">k</span></code>gets 1 sequence and <code class="docutils literal notranslate"><span class="pre">l</span></code> gets two sequences.
If all adapters should get the same batch size this can be specified by passing one batch size e.g. <code class="docutils literal notranslate"><span class="pre">batch_sizes</span> <span class="pre">=</span> <span class="pre">2</span></code>. The sum
specified batch has to match the batch size of the input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers.adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="o">//</span> <span class="o">...</span>

<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;l&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">BatchSplit</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="s2">&quot;l&quot;</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

</pre></div>
</div>
</div>
<div class="section" id="parallel">
<h2><code class="docutils literal notranslate"><span class="pre">Parallel</span></code><a class="headerlink" href="#parallel" title="Permalink to this headline">¶</a></h2>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/parallel.png
    :height: 300
    :align: center
    :alt: Illustration of parallel adapter inference.

    Parallel adapter inference as implemented by the &#39;Parallel&#39; block. The input is replicated at the first layer with parallel adapters.
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Parallel</span></code> block can be used to enable parallel multi-task inference on different adapters, each with their own prediction head.
Parallel adapter inference was first used in <em>AdapterDrop: On the Efficiency of Adapters in Transformers</em> <a class="reference external" href="https://arxiv.org/pdf/2010.11918.pdf">(Rücklé et al., 2020)</a>.</p>
<p>In the following example, we load two adapters for semantic textual similarity (sts) from the Hub, one trained on the STS benchmark, the other trained on the MRPC dataset.
We activate a parallel setup where the input is passed through both adapters and their respective prediction heads.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithHeads</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="n">adapter1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;sts/sts-b@ukp&quot;</span><span class="p">)</span>
<span class="n">adapter2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;sts/mrpc@ukp&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">Parallel</span><span class="p">(</span><span class="n">adapter1</span><span class="p">,</span> <span class="n">adapter2</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Adapters are great!&quot;</span><span class="p">,</span> <span class="s2">&quot;Adapters are awesome!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">output1</span><span class="p">,</span> <span class="n">output2</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_ids</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;STS-B adapter output:&quot;</span><span class="p">,</span> <span class="n">output1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MRPC adapter output:&quot;</span><span class="p">,</span> <span class="nb">bool</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">Parallel</span></code> block is only intended for <em>inference</em>, not for <em>training</em> adapters.</p>
</div>
<div class="section" id="nesting-composition-blocks">
<h2>Nesting composition blocks<a class="headerlink" href="#nesting-composition-blocks" title="Permalink to this headline">¶</a></h2>
<p>Of course, it is also possible to combine different composition blocks in one adapter setup.
E.g., we can nest a <code class="docutils literal notranslate"><span class="pre">Split</span></code> block within a <code class="docutils literal notranslate"><span class="pre">Stack</span></code> of adapters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers.adapters.composition</span> <span class="k">as</span> <span class="nn">ac</span>

<span class="n">model</span><span class="o">.</span><span class="n">active_adapters</span> <span class="o">=</span> <span class="n">ac</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">ac</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="n">split_index</span><span class="o">=</span><span class="mi">60</span><span class="p">))</span>
</pre></div>
</div>
<p>However, combinations of adapter composition blocks cannot be arbitrarily deep. All currently supported possibilities are visualized in the figure below.</p>
<div class="highlight-eval_rst notranslate"><div class="highlight"><pre><span></span>.. figure:: img/adapter_blocks_nesting.png
    :height: 300
    :align: center
    :alt: Adapter composition block combinations

    Allowed nestings of adapter composition blocks.
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="training.html" class="btn btn-neutral float-right" title="Adapter Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="adapters.html" class="btn btn-neutral float-left" title="Introduction to Adapters" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020-2022, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: v2.3.0
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt><dd><a href="../v1.1.1/index.html">v1.1.1</a></dd><dd><a href="adapter_composition.html">v2.3.0</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../index.html">master</a></dd>
    </dl>
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>